# -*- coding: utf-8 -*-
"""Untitled15.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1WNRgSlhZAwALcQikY-ComhYsszCBZyUE
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.datasets import make_classification
from sklearn.linear_model import LogisticRegression, SGDClassifier
from mlxtend.plotting import plot_decision_regions
from sklearn.utils import shuffle

https://drive.google.com/file/d/1Won6xkyYCcJLJ7eMpVt5VA_4P0tE1nb7/view?usp=sharing

!pip install --upgrade --no-cache-dir gdown
!gdown 1Won6xkyYCcJLJ7eMpVt5VA_4P0tE1nb7

df = pd.read_csv('/content/data_banknote_authentication.txt')
df

shuffled_data = shuffle(df)
shuffled_data.to_csv('created_data.csv', index=False)
print(shuffled_data)

df1 = pd.read_csv('/content/created_data.csv')
df1

"""Logistic Regression (from Scratch)"""

def sigmoid(x):
    return 1 / (1 + np.exp(-x))

def logistic_regression(x, w):
    y_hat = sigmoid(x @ w)
    return y_hat

"""Binary Cross Entropy (BCE)"""

def bce(y, y_hat):
    loss = -(np.mean(y*np.log(y_hat) + (1-y)*np.log(1-y_hat)))
    return loss

"""Gradient"""

def gradient(x, y, y_hat):
    grads = (x.T @ (y_hat - y)) / len(y)
    return grads

"""Gradient Descent"""

def gradient_descent(w, eta, grads):
    w -= eta*grads
    return w

"""Accuracy"""

def accuracy(y, y_hat):
    acc = np.sum(y == np.round(y_hat)) / len(y)
    return acc

"""train آموزش داده های"""

X = df1[['x1','x2','x3','x4']].values
y = df1[['y']].values
X, y

"""تقسیم داده ها به دو دسته آموزش و ارزیابی"""

x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.2,random_state=42)

x_train.shape, x_test.shape, y_train.shape, y_test.shape

y_hat = logistic_regression(x_train, np.random.randn(4, 1))
print(y_hat.shape)

x_train = np.hstack((np.ones((len(x_train), 1)), x_train))
x_train.shape

m = 4
w = np.random.randn(m+1, 1)
print(w.shape)

eta = 0.01
n_epochs = 60000 #N

error_hist = []

for epoch in range(n_epochs):
    # predictions
    y_hat = logistic_regression(x_train, w)

    # loss
    e = bce(y_train, y_hat)
    error_hist.append(e)

    # gradients
    grads = gradient(x_train, y_train, y_hat)

    # gradient descent
    w = gradient_descent(w, eta, grads)

    if (epoch+1) %  1== 0:
        print(f'Epoch={epoch}, \t E={e:.4},\t w={w.T[0]}')

plt.plot(error_hist)

y_hat = logistic_regression(x_train, w)
accuracy(y_train, y_hat)

x_test = np.hstack((np.ones((len(x_test), 1)), x_test))
y_hat = logistic_regression(x_test, w)
accuracy(y_test, y_hat)