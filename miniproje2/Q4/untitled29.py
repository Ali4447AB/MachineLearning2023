# -*- coding: utf-8 -*-
"""Untitled29.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/17sD601WK1dpuF0j-W0eO0kWXONC_F0oC
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import random

from sklearn.model_selection import train_test_split
from sklearn import preprocessing
from sklearn.preprocessing import MinMaxScaler, LabelEncoder
from sklearn.metrics import r2_score

import tensorflow as tf
from tensorflow import keras
from keras import preprocessing
from keras.models import Sequential
from keras.layers import Dense

import warnings
warnings.filterwarnings("ignore")

https://drive.google.com/file/d/1y4gApyw8HURa-j-SNiTWVkzUw8czxOZN/view?usp=sharing

!pip install --upgrade --no-cache-dir gdown
!gdown 1y4gApyw8HURa-j-SNiTWVkzUw8czxOZN

# Load the dataset from the specified file path
df = pd.read_csv('/content/data.csv')

# Display the first few rows of the DataFrame
df.head()

# Display information about the DataFrame
df.info()

df.isnull().sum()

# Extract the "CarName" column
date = df["date"]

# Count the occurrences of each unique car name
date.value_counts()

# تبدیل ستون 'date' به نوع داده datetime
df['date'] = pd.to_datetime(df['date'])

# اضافه کردن ستون 'ماه'
df['month'] = df['date'].dt.month

# اضافه کردن ستون 'سال'
df['year'] = df['date'].dt.year

# نمایش DataFrame جدید
print(df)

# Drop the specified columns from the DataFrame
df = df.drop(['date', 'country', 'year','street', 'statezip'], axis=1)

# List of specified categorical columns
dummy = ['city']

# Convert categorical columns to numerical using one-hot encoding
df2 = pd.get_dummies(df, columns=dummy, drop_first=True)

# Display the first few rows of the modified DataFrame
df2.head()

# Calculate the correlation between columns and 'price', then sort them in descending order
correlation_matrix = df2.corr()['price'].sort_values(ascending=False)
correlation_matrix

# Plot a heatmap to visualize the correlation matrix
plt.figure(figsize=(20, 20))
sns.heatmap(df2.corr(), cmap="RdYlGn")
plt.show()

# Select columns with numerical data types
num = df.select_dtypes(exclude=['object']).columns
num

# Create a heatmap to visualize the correlation matrix of numerical columns
plt.figure(figsize=(15, 15))
sns.heatmap(df[num].corr(), annot=True, cmap='inferno', mask=np.triu(df[num].corr(), k=1))

# Create a scatter plot of sqft_living against price
plt.figure(figsize=(5, 5))
plt.scatter(x='sqft_living', y='price', data=df2)
plt.xlabel('sqft_living')
plt.title('Sqft_Living vs. Price')
plt.ylabel('price')
plt.show()

# Create a 4x4 grid of subplots for various numerical variables
plt.figure(figsize=(20, 20))

plt.subplot(4,4,1)
sns.distplot(df['bedrooms'], color="red").set_title('bedrooms Interval')

plt.subplot(4,4,2)
sns.distplot(df['bathrooms'], color="green").set_title('bathrooms Interval')

plt.subplot(4,4,3)
sns.distplot(df['sqft_living'], color="black").set_title('sqft_living Interval')

plt.subplot(4,4,4)
sns.distplot(df['sqft_lot'], color="blue").set_title('sqft_lot Interval')

plt.subplot(4,4,5)
sns.distplot(df['floors'], color="red").set_title('floors Interval')

plt.subplot(4,4,6)
sns.distplot(df['waterfront'], color="green").set_title('waterfront Interval')

plt.subplot(4,4,7)
sns.distplot(df['view'], color="black").set_title('view Interval')

plt.subplot(4,4,8)
sns.distplot(df['condition'], color="blue").set_title('condition Interval')

plt.subplot(4,4,9)
sns.distplot(df['sqft_above'], color="red").set_title('sqft_above Interval')

plt.subplot(4,4,10)
sns.distplot(df['sqft_basement'], color="green").set_title('sqft_basement Interval')

plt.subplot(4,4,11)
sns.distplot(df['yr_built'], color="black").set_title('yr_built Interval')

plt.subplot(4,4,12)
sns.distplot(df['yr_renovated'], color="blue").set_title('yr_renovated Interval')

plt.subplot(4,4,12)
sns.distplot(df['month'], color="blue").set_title('month Interval')

plt.subplot(4,4,13)
sns.distplot(df['price'], color="red").set_title('price Interval')

# Initialize LabelEncoder
l1 = LabelEncoder()

# Convert object-type columns to numerical using Label Encoding
for i in df2.columns:
    if df2[i].dtype == 'object':
        df2[i] = l1.fit_transform(df2[i])

df2

# Separate input (X) and output (Y) data
X = df2.drop(["price"], axis=1)  # Input data
Y = df2["price"]                  # Output data

# Perform train-test split
x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=7)

# Print the shapes of the datasets
print("X Train Scaler : ", x_train.shape) # Print shape of x_train
print("X Test Scaler : ",  x_test.shape)  # Print shape of x_test
print("Y Train Scaler : ", y_train.shape) # Print shape of y_train
print("Y Test Scaler : ",  y_test.shape)  # Print shape of y_test

# Find the maximum value in y_train data
max_1 = y_train.values
maximum = 0
minimum = min(y_train)

# Loop through the elements of y_train
for i in range(x_train.shape[0]):
    if max_1[i] > maximum:
        maximum = max_1[i]

# Initialize Min-Max Scaler
scaler_1 = MinMaxScaler()

# Normalize the training input data
x_train = scaler_1.fit_transform(x_train)

# Normalize the test input data
x_test = scaler_1.transform(x_test)

# Convert y_train and y_test type to DataFrame
y_train = pd.DataFrame(y_train)
y_test = pd.DataFrame(y_test)

scaler_2 = MinMaxScaler()

# Normalize outputs
y_train = scaler_2.fit_transform(y_train)
y_test = scaler_2.transform(y_test)

model_2 = Sequential()

# Add the first hidden layer with 50 neurons and linear activation function
model_2.add(Dense(50, activation='linear', input_shape=(x_train.shape[1],)))

# Add the second hidden layer with 30 neurons and linear activation function
model_2.add(Dense(30, activation='linear'))

# Add an output layer with 1 neuron and linear activation function
model_2.add(Dense(1, activation='linear'))

model_2.summary()

model_2.compile(optimizer='adam', loss='mse')
history = model_2.fit(x_train, y_train, validation_split=0.15, epochs=300 ,batch_size=100, verbose=0)

#Evaluate the model
loss = model_2.evaluate(x_test , y_test)

y_pred_2 = model_2.predict(x_test)
rscore_2 = r2_score(y_test , y_pred_2)

rscore_2

# Plot the training and validation loss
plt.plot(history.history['loss'], label='train')   # Training loss
plt.plot(history.history['val_loss'], label='val')  # Validation loss

plt.legend(['Training Loss', 'Validation Loss'])
plt.xlabel("Epochs")
plt.ylabel("Loss")
plt.show()

random_pred = list()
random_test = list()
y_test_array = np.array(y_test)
for i in range(5):
  j = random.randint(0, len(y_pred_2))
  random_pred.append(y_pred_2[i])
  random_test.append(y_test_array[i])

# Plot the random predictions and actual test outputs
plt.plot(random_pred, 'b', label='Prediction')  # Blue line for predictions
plt.plot(random_test, 'r', label='Test')        # Red line for actual test outputs

plt.legend()
plt.grid()
plt.show()